train <- read.csv("C:/Users/hkamel/Documents/Data Science course/data/train.csv", stringsAsFactors = TRUE)
test <- read.csv("C:/Users/hkamel/Documents/Data Science course/data/test.csv", stringsAsFactors = TRUE)


# Install required packages
packages <- c('ggplot2','tidyverse','ggcorrplot','scales','randomForest','caret')

installed_packages <- packages %in% rownames(install.packages())
if (any(installed_packages == FALSE)){
  install.packages(packages[!installed_packages])
}

#Load the packages
invisible(lapply(packages, library,character.only=TRUE))

attach(train)
dim(train)
[1] 381109     12
summary(train)


# See structure of data
str(train)

# Check for missing values. No missing values found
sapply(train, function(x) sum(is.na(x)))


#converting integer variables to a numeric variables
train$Age <- as.numeric(train$Age)
train$Vintage <- as.numeric(train$Vintage)
train <- train[,-1]

#Convert columns with text values in Training set
train$Vehicle_Age <- ifelse(train$Vehicle_Age == "< 1 Year",0,
                            ifelse(train$Vehicle_Age == "1-2 Year",1,
                                   ifelse(train$Vehicle_Age == "> 2 Years",2,FALSE)))
train$Vehicle_Age <- as.factor(train$Vehicle_Age)

train$Gender <- ifelse(train$Gender == "Male",1,0)
train$Gender <- as.factor(train$Gender)

train$Vehicle_Damage <- ifelse(train$Vehicle_Damage == "Yes",1,0)
train$Vehicle_Damage <- as.factor(train$Vehicle_Damage)
train$Previously_Insured <- as.factor(train$Previously_Insured)
train$Driving_License <-as.factor(train$Driving_License)
train$Response <- as.factor(train$Response)

# Data visualization
options(scipen = 999) ### turn off scientific notation like 1e-09


ggplot(data = train,aes(x=Response)) + 
  geom_bar(aes(fill=Gender),position = "dodge") + 
  scale_fill_manual(values=c("#999999", "#E69F00"))

ggplot(data = train,aes(x=Response)) + 
  geom_bar(aes(fill=Vehicle_Damage),position="dodge") +
  scale_fill_manual(values=c("#999999", "#E69F00"))

ggplot(data = train,aes(x=Response)) + 
  geom_bar(aes(fill=Previously_Insured),position = "dodge") +
  scale_fill_manual(values=c("#999999", "#E69F00"),labels = c("No", "Yes"))

ggplot(data=train,aes(x=Response,y=Age,color=Response)) +
  geom_boxplot(outlier.colour="black", outlier.shape=16,
               outlier.size=2, notch=FALSE)


ggplot(data = train,aes(x=Age)) + 
  geom_histogram(aes(y=..density..),colour="white",binwidth=2,fill=..density..) +  
  geom_density(alpha=.2, fill="#FF6666",)

chisq.test(Response,Gender)
chisq.test(Response,Vehicle_Damage)
chisq.test(Response,Previously_Insured)
chisq.test(Response,Vehicle_Age)
chisq.test(Response,Driving_License)


# Correlation plots
library(corrplot)
library(RColorBrewer)
corr <- cor(train[,-11])
corrplot(corr,type="lower",method="number")


# Imbalance dataset with 87.7% of customer with no interest and 12.3% of customer w/ interest

table(train$Response)
  0      1 
334399  46710 

prop.table(table(train$Response))
  0         1 
0.8774366 0.1225634 

# A contingency table between Response & Gender (0=Female, 1=Male)
round((prop.table(table(train$Response,train$Gender))),2)
    0    1
0 0.41 0.47
1 0.05 0.07

# A contingency table between Response & Vehicle Damage (0=No, 1=Yes)
round((prop.table(table(train$Response,train$Vehicle_Damage))),2)
    0    1
0 0.49 0.38
1 0.00 0.12

# A contingency table between Response & Previously Insured (0=No, 1=Yes)
round((prop.table(table(train$Response,train$Previously_Insured))),2)
    0    1
0 0.42 0.46
1 0.12 0.00

# A contingency table between Response & Driving License (0=No, 1=Yes)
round((prop.table(table(train$Response,train$Driving_License))),2)
    0    1
0 0.00 0.88
1 0.00 0.12


# Split train dataset to training & testing set
set.seed(100)
n <- nrow(train)
ntrain <- round(n*0.7)
tindex <- sample(n,ntrain)

trainData <- train[tindex,]
testData <- train[-tindex,]
y_actual <- testData$Response


# There is about 88% of the response is no and only 12% of the response is yes.
# Clearly there is a class imbalance.
prop.table(table(trainData$Response))
  0         1 
0.8770617 0.1229383 

install.packages("ROSE")
library(ROSE)

#downsampling data

set.seed(100)
down_train <- ovun.sample(Response~., data=trainData,
                          p=0.5,seed=1, method="under")$data

table(down_train$Response)
0     1 
32891 32797 

#UpSampling data
set.seed(100)
up_train <- ovun.sample(Response~., data=trainData,
                          p=0.5,seed=1, method="over")$data

table(up_train$Response)
0      1 
233979 234243 

#Both
both_train <- ovun.sample(Response~., data=trainData,
                                N=nrow(trainData), p=0.5, 
                                seed=1, method="both")$data
table(both_train$Response)
0      1 
133198 133578

# ROSE
set.seed(100)
train.rose <- ROSE(Response~., data  = trainData)$data                    
table(train.rose$Response) 
  0      1 
133612 133164 


formula <- Response ~ Gender + Age + Driving_License + Region_Code + Previously_Insured + Vehicle_Age + Vehicle_Damage +
           Annual_Premium + Policy_Sales_Channel + Vintage
formula.down <- Class ~ Gender + Age + Driving_License + Region_Code + Previously_Insured + Vehicle_Age + Vehicle_Damage +
  Annual_Premium + Policy_Sales_Channel + Vintage

install.packages("rpart")
library(rpart)

# Imbalanced dataset
tree.fit <- rpart(Response~., data=trainData)
tree.prediction <- predict(tree.fit,newdata=testData,type="class")
table(y_actual,tree.prediction)
roc.curve(y_actual,tree.prediction)

# Building the Decision tree Model 
tree.fit.down <- rpart(Response~., data=down_train)
tree.fit.up <- rpart(Response~., data=up_train)
tree.fit.rose <- rpart(Response~., data=train.rose)
tree.fit.both <- rpart(Response~., data=both_train)

# make predictions on test data
tree.prediction.both <- predict(tree.fit.both,newdata=testData)
tree.prediction.down <- predict(tree.fit.down,newdata=testData)
tree.prediction.up <- predict(tree.fit.up,newdata=testData)
tree.prediction.rose <- predict(tree.fit.rose,newdata=testData)

# AUC score
par(mfrow=c(2,2))
roc.curve(y_actual,tree.prediction.both[,2],col = "RED", main = "ROC curve of Combination")
roc.curve(y_actual,tree.prediction.rose[,2],col = "ORANGE", main = "ROC curve of ROSE")
roc.curve(y_actual,tree.prediction.down[,2],col = "BLUE", main = "ROC curve of DownSample")
roc.curve(y_actual,tree.prediction.up[,2],col = "BLUE", main = "ROC curve of UpSample")

#Building logistic regression model using Upsampling data

glm.fit <- glm(Response~., data=train.rose,family=binomial)
prediction <- predict(glm.fit,newdata = testData,type="response")

# Model diagnostic
summary(glm.fit)


install.packages("InformationValue")
library(InformationValue)
plotROC(y_actual,prediction)

# misclassification error

misClassError(y_actual, prediction, threshold = 0.5)
0.3616


# Confusion Matrix
confusionMatrix(y_actual,prediction,threshold=0.5)
    0     1
0 59440   368
1 40980 13545

specificity(y_actual,prediction,threshold = 0.5)
0.591914

sensitivity(y_actual,prediction,threshold=0.5)
0.9735499

accuracy.meas(y_actual,prediction,threshold = 0.5)

Call: 
  accuracy.meas(response = y_actual, predicted = prediction, threshold = 0.5)

Examples are labelled as positive when predicted is greater than 0.5 

precision: 0.249
recall: 0.977
F: 0.198
